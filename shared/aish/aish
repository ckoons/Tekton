#!/usr/bin/env python3
"""
aish - The AI Shell
Main entry point for the AI shell interpreter.
"""

import sys
import os
import argparse
from pathlib import Path


# Get the real path of the script (follows symlinks)
script_path = Path(__file__).resolve()
aish_root = script_path.parent
src_path = aish_root / 'src'

# Add src to path
sys.path.insert(0, str(src_path))

# Also add Tekton root if available (we're inside Tekton/shared/aish)
tekton_root = aish_root.parent.parent  # Go up to Tekton root
if tekton_root.exists() and (tekton_root / 'shared').exists():
    sys.path.insert(0, str(tekton_root))

from core.shell import AIShell
from core.version import __version__

def main():
    parser = argparse.ArgumentParser(
        description='aish - The AI Shell\n\nProvides AI capabilities in your terminal through the Tekton platform.',
        epilog='''Examples:
  aish                      # Start interactive AI shell
  aish apollo "question"    # Send question to Apollo AI
  echo "data" | aish athena # Pipe data to Athena AI
  aish team-chat "hello"    # Broadcast to all AIs
  aish -l                   # List available AIs
  aish script.ai            # Run AI script file

Available AIs: numa, tekton, prometheus, telos, metis, harmonia,
               synthesis, athena, sophia, noesis, engram, apollo,
               rhetor, penia, hermes, ergon, terma

Environment variables:
  RHETOR_ENDPOINT    # Override default Rhetor endpoint
  AISH_ACTIVE        # Set by aish-proxy when active
  AISH_DEBUG         # Enable debug output''',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument(
        'ai_or_script',
        nargs='?',
        help='AI name (apollo, athena, etc.) or script file'
    )
    
    parser.add_argument(
        'message',
        nargs='*',
        help='Message to send to AI'
    )
    
    parser.add_argument(
        '-c', '--command',
        help='Execute a single AI pipeline command and exit'
    )
    
    parser.add_argument(
        '-v', '--version',
        action='version',
        version=f'aish {__version__}'
    )
    
    parser.add_argument(
        '-d', '--debug',
        action='store_true',
        help='Enable debug output'
    )
    
    parser.add_argument(
        '-r', '--rhetor',
        default=None,
        help='Rhetor endpoint (default: TEKTON_RHETOR_PORT env var or http://localhost:8003)'
    )
    
    parser.add_argument(
        '-l', '--list-ais',
        action='store_true',
        help='List available AI specialists and exit'
    )
    
    args = parser.parse_args()
    
    # Create shell instance
    shell = AIShell(
        rhetor_endpoint=args.rhetor,
        debug=args.debug
    )
    
    # Handle general help command
    if args.ai_or_script == "help":
        print("Usage: aish [options] [component] [command/message]")
        tekton_root = os.environ.get('TEKTON_ROOT', '/Users/cskoons/projects/github/Tekton')
        print(f"AI Training: {tekton_root}/MetaData/TektonDocumentation/AITraining/aish/")
        print(f"User Guides: {tekton_root}/MetaData/TektonDocumentation/UserGuides/aish/")
        return
    
    # Handle special commands
    if args.list_ais:
        shell._list_ais()
        return
    
    # Known AI names with their roles (all lowercase for comparison)
    ai_names = ['numa',        # Companion
                'tekton',      # Projects
                'prometheus',  # Planning
                'telos',       # Requirements
                'metis',       # Workflows
                'harmonia',    # Orchestration
                'synthesis',   # Integration
                'athena',      # Knowledge
                'sophia',      # Learning
                'noesis',      # Discovery
                'engram',      # Memory
                'apollo',      # Attention/Prediction
                'rhetor',      # LLM/Prompt/Context
                'penia',       # LLM Cost
                'hermes',      # Messages/Data
                'ergon',       # Agents/Tools/MCP
                'terma',       # Terminal
                'team-chat']   # Broadcast to all
    
    # Check if first argument is an AI name (case-insensitive)
    if args.ai_or_script and args.ai_or_script.lower() in ai_names:
        component = args.ai_or_script.lower()
        
        
        # Special handling for terma commands
        if component == 'terma':
            # Import and use terma handler
            from commands.terma import handle_terma_command
            return handle_terma_command(args.message)
        
        # Check for help command
        if args.message and args.message[0] == "help":
            print(f"Usage: aish {component} [message]")
            tekton_root = os.environ.get('TEKTON_ROOT', '/Users/cskoons/projects/github/Tekton')
            print(f"AI Training: {tekton_root}/MetaData/TektonDocumentation/AITraining/{component}/")
            print(f"User Guides: {tekton_root}/MetaData/TektonDocumentation/UserGuides/{component}/")
            return
        
        
        # Get input from message args first, then stdin, then interactive
        if args.message:
            # Message provided as arguments - this takes priority
            input_data = ' '.join(args.message).strip()
            if args.debug:
                print(f"[DEBUG] Using message args: {args.message}")
                print(f"[DEBUG] Joined as: '{input_data}'")
        elif not sys.stdin.isatty():
            # Data might be piped in
            # Try to read with a very short timeout to avoid hanging
            import select
            if select.select([sys.stdin], [], [], 0.0)[0]:
                input_data = sys.stdin.read().strip()
                if args.debug:
                    print(f"[DEBUG] Got piped input: '{input_data}'")
            else:
                # No data in pipe, treat as interactive
                print(f"[aish] Entering interactive mode with {component}")
                print("Type your message and press Ctrl+D when done:")
                input_data = sys.stdin.read().strip()
        else:
            # Interactive mode with specific AI
            print(f"[aish] Entering interactive mode with {component}")
            print("Type your message and press Ctrl+D when done:")
            input_data = sys.stdin.read().strip()
        
        # Check if we actually have data
        if not input_data:
            print(f"aish: No message provided for {component}", file=sys.stderr)
            sys.exit(1)
        
        # Send directly to AI (no synthetic pipeline)
        if component == 'team-chat':
            shell.broadcast_message(input_data)
        else:
            shell.send_to_ai(component, input_data)
    
    # Original behavior for other cases
    elif args.command:
        # Single command mode
        shell.execute_command(args.command)
    elif args.ai_or_script and os.path.isfile(args.ai_or_script):
        # Script execution mode
        shell.execute_script(args.ai_or_script)
    elif args.ai_or_script:
        # Unknown AI or command
        print(f"aish: Unknown AI or file: {args.ai_or_script}", file=sys.stderr)
        print("Available AIs: " + ", ".join(ai_names), file=sys.stderr)
        sys.exit(1)
    else:
        # Interactive mode
        shell.interactive()

if __name__ == '__main__':
    main()