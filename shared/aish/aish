#!/usr/bin/env python3
"""
aish - The AI Shell
Main entry point for the AI shell interpreter.
"""

import sys
import os
import argparse
from pathlib import Path

# Try to import landmarks if available
try:
    from landmarks import architecture_decision, integration_point, api_contract, state_checkpoint
except ImportError:
    # Landmarks not available, create no-op decorators
    def architecture_decision(**kwargs):
        def decorator(func):
            return func
        return decorator
    
    def integration_point(**kwargs):
        def decorator(func):
            return func
        return decorator
    
    def api_contract(**kwargs):
        def decorator(func):
            return func
        return decorator
    
    def state_checkpoint(**kwargs):
        def decorator(func):
            return func
        return decorator


# Get the real path of the script (follows symlinks)
script_path = Path(__file__).resolve()
aish_root = script_path.parent
src_path = aish_root / 'src'

# Add src to path
sys.path.insert(0, str(src_path))

# Add parent paths to find shared module (we're in $TEKTON_ROOT/shared/aish)
tekton_root_for_import = aish_root.parent.parent  # Go up to $TEKTON_ROOT
if tekton_root_for_import.exists() and (tekton_root_for_import / 'shared').exists():
    sys.path.insert(0, str(tekton_root_for_import))

# Perform TektonEnvironLock.load() to ensure TektonEnviron is available
from shared.env import TektonEnvironLock, TektonEnviron
TektonEnvironLock.load() 

# Trust TEKTON_ROOT from environment
env_tekton_root = TektonEnviron.get('TEKTON_ROOT')
if not env_tekton_root:
    print(f"Error: TEKTON_ROOT environment variable not set")
    print(f"Please set TEKTON_ROOT to your Tekton installation directory")
    sys.exit(1)

tekton_root = Path(env_tekton_root)

# Verify that aish exists at the expected location
expected_aish = tekton_root / 'shared' / 'aish' / 'aish'
if not expected_aish.exists():
    print(f"Error: aish not found at expected location")
    print(f"  TEKTON_ROOT: {tekton_root}")
    print(f"  Expected aish at: {expected_aish}")
    print(f"  Please ensure TEKTON_ROOT points to a valid Tekton installation")
    sys.exit(1)

# Add tekton_root to path for imports
if tekton_root.exists() and (tekton_root / 'shared').exists():
    sys.path.insert(0, str(tekton_root))

from core.shell import AIShell
from core.version import __version__

# Start MCP server if not disabled
if not os.environ.get('AISH_NO_MCP'):
    try:
        # Check if MCP server is already running
        import socket
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        mcp_port = int(TektonEnviron.get("AISH_MCP_PORT", "3100"))
        result = sock.connect_ex(('localhost', mcp_port))
        sock.close()
        
        if result != 0:  # Port not in use, start MCP server
            # Use subprocess to start MCP server
            import subprocess
            import atexit
            
            mcp_script = aish_root / 'aish-mcp'
            if mcp_script.exists():
                # Start MCP server as a separate process
                env = os.environ.copy()
                env['TEKTON_ROOT'] = str(tekton_root)
                
                mcp_process = subprocess.Popen(
                    [sys.executable, str(mcp_script)],
                    env=env,
                    stdout=subprocess.DEVNULL if not os.environ.get('AISH_DEBUG_MCP') else None,
                    stderr=subprocess.DEVNULL if not os.environ.get('AISH_DEBUG_MCP') else None,
                    start_new_session=True  # Detach from parent
                )
                
                # Don't register cleanup - let it run independently
                # atexit.register(lambda: mcp_process.terminate() if mcp_process.poll() is None else None)
                
                if os.environ.get('AISH_DEBUG_MCP'):
                    print(f"[DEBUG] Started MCP server process PID: {mcp_process.pid}", file=sys.stderr)
                
                # Give it time to start
                import time
                time.sleep(1)
            else:
                print(f"Warning: MCP script not found at {mcp_script}", file=sys.stderr)
        else:
            if os.environ.get('AISH_DEBUG_MCP'):
                print(f"[DEBUG] MCP server already running on port {mcp_port}", file=sys.stderr)
    except Exception as e:
        # Don't fail if MCP can't start - aish should still work
        print(f"Warning: Could not start MCP server: {e}", file=sys.stderr)
        if os.environ.get('AISH_DEBUG_MCP'):
            import traceback
            traceback.print_exc(file=sys.stderr)


@integration_point(
    title="Command Type Dispatch Router",
    description="Routes commands to appropriate handlers based on type",
    target_component="command_handlers",
    protocol="function_call",
    data_flow="command_type → import handler → execute command",
    integration_date="2025-01-18"
)
def _dispatch_command(command_type, args, shell):
    """Dispatch command to appropriate handler based on type."""
    if command_type == 'forward':
        from commands.forward import handle_forward_command
        return handle_forward_command(args.message or [])
    elif command_type == 'unforward':
        from commands.forward import handle_unforward_command
        return handle_unforward_command(args.message or [])
    elif command_type == 'prompt':
        from commands.prompt import handle_prompt_command
        return handle_prompt_command(args.message or [])
    elif command_type == 'purpose':
        from commands.purpose import handle_purpose_command
        return handle_purpose_command(args.message or [])
    elif command_type == 'terma':
        from commands.terma import handle_terma_command
        return handle_terma_command(args.message)
    elif command_type == 'review':
        from commands.review import handle_command as handle_review_command
        return handle_review_command(args.message or [])
    elif command_type == 'project':
        from commands.project import handle_project_command
        return handle_project_command(args.message or [])
    # Claude Code IDE Command Integration - CI tooling
    # These commands provide Companion Intelligences with IDE-like capabilities
    # to eliminate guessing and save ~40% context during development.
    elif command_type == 'introspect':
        from commands.introspect import introspect_command
        result = introspect_command(args.message or [], shell)
        print(result)
        return True
    elif command_type == 'context':
        from commands.context import context_command
        result = context_command(args.message or [], shell)
        print(result)
        return True
    elif command_type == 'explain':
        from commands.explain import explain_command
        result = explain_command(args.message or [], shell)
        print(result)
        return True
    elif command_type == 'route':
        from commands.route import handle_route_command
        # Let route command handle its own argument parsing
        return handle_route_command()
    return False


@api_contract(
    title="AI Message Input Collection",
    description="Collects input with priority: args > piped stdin > interactive",
    endpoint="internal",
    method="function",
    request_schema={"args": "argparse.Namespace", "component": "string"},
    response_schema={"input_data": "string"},
    performance_requirements="<10ms for args, <100ms for stdin"
)
def _collect_ai_input(args, component):
    """Collect input for AI from various sources."""
    # Get input from message args first, then stdin, then interactive
    if args.message:
        # Message provided as arguments - this takes priority
        input_data = ' '.join(args.message).strip()
        if args.debug:
            print(f"[DEBUG] Using message args: {args.message}")
            print(f"[DEBUG] Joined as: '{input_data}'")
    elif not sys.stdin.isatty():
        # Data might be piped in
        # Try to read with a very short timeout to avoid hanging
        import select
        if select.select([sys.stdin], [], [], 0.0)[0]:
            input_data = sys.stdin.read().strip()
            if args.debug:
                print(f"[DEBUG] Got piped input: '{input_data}'")
        else:
            # No data in pipe, treat as interactive
            print(f"[aish] Entering interactive mode with {component}")
            print("Type your message and press Ctrl+D when done:")
            input_data = sys.stdin.read().strip()
    else:
        # Interactive mode with specific AI
        print(f"[aish] Entering interactive mode with {component}")
        print("Type your message and press Ctrl+D when done:")
        input_data = sys.stdin.read().strip()
    
    return input_data


@integration_point(
    title="AI Communication Handler",
    description="Routes messages to team-chat or individual AIs",
    target_component="ai_shell",
    protocol="internal_api",
    data_flow="command → shell.send_to_ai or shell.broadcast_message",
    integration_date="2025-01-18"
)
def _handle_ai_command(args, shell, ai_names):
    """Handle command directed at a specific AI."""
    component = args.ai_or_script.lower()
    
    # Special handling for terma commands
    if component == 'terma':
        _dispatch_command('terma', args, shell)
        return
    
    # Check for help command
    if args.message and args.message[0] == "help":
        print(f"Usage: aish {component} [message]")
        tekton_root = TektonEnviron.get('TEKTON_ROOT', '/Users/cskoons/projects/github/Tekton')
        print(f"AI Training: {tekton_root}/MetaData/TektonDocumentation/AITraining/{component}/")
        print(f"User Guides: {tekton_root}/MetaData/TektonDocumentation/UserGuides/{component}/")
        return
    
    # Collect input from various sources
    input_data = _collect_ai_input(args, component)
    
    # Check if we actually have data
    if not input_data:
        print(f"aish: No message provided for {component}", file=sys.stderr)
        sys.exit(1)
    
    # Send directly to AI (no synthetic pipeline)
    if component == 'team-chat':
        shell.broadcast_message(input_data)
    else:
        shell.send_to_ai(component, input_data)


@architecture_decision(
    title="aish Command Entry Point",
    description="Main orchestration point for all aish commands",
    rationale="Provides unified entry for AI shell, scripts, and individual AI commands",
    alternatives_considered=["Separate binaries per AI", "Menu-driven interface"],
    impacts=["usability", "consistency", "extensibility"],
    decided_by="Casey",
    decision_date="2025-01-18"
)
def main():
    parser = argparse.ArgumentParser(
        description='aish - The AI Shell\n\nProvides AI capabilities in your terminal through the Tekton platform.',
        epilog='''Examples:
  aish                      # Start interactive AI shell
  aish apollo "question"    # Send question to Apollo AI
  echo "data" | aish athena # Pipe data to Athena AI
  aish team-chat "hello"    # Broadcast to all AIs
  aish -l                   # List available AIs
  aish --capture numa "hi"  # Capture output to .tekton/aish/captures/
  aish status               # Show system status
  aish status json          # JSON formatted status
  aish script.ai            # Run AI script file

Available AIs: numa, tekton, prometheus, telos, metis, harmonia,
               synthesis, athena, sophia, noesis, engram, apollo,
               rhetor, penia, hermes, ergon, terma

Environment variables:
  RHETOR_ENDPOINT    # Override default Rhetor endpoint
  AISH_ACTIVE        # Set by aish-proxy when active
  AISH_DEBUG         # Enable debug output
  AISH_NO_MCP        # Disable MCP server startup''',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument(
        'ai_or_script',
        nargs='?',
        help='AI name (apollo, athena, etc.) or script file'
    )
    
    parser.add_argument(
        'message',
        nargs='*',
        help='Message to send to AI'
    )
    
    parser.add_argument(
        '-c', '--command',
        help='Execute a single AI pipeline command and exit'
    )
    
    parser.add_argument(
        '-v', '--version',
        action='version',
        version=f'aish {__version__}'
    )
    
    parser.add_argument(
        '-d', '--debug',
        action='store_true',
        help='Enable debug output'
    )
    
    parser.add_argument(
        '-r', '--rhetor',
        default=None,
        help='Rhetor endpoint (default: TEKTON_RHETOR_PORT env var or http://localhost:8003)'
    )
    
    parser.add_argument(
        '-l', '--list-ais',
        action='store_true',
        help='List available AI specialists and exit'
    )
    
    parser.add_argument(
        '--capture',
        action='store_true',
        help='Capture command output to .tekton/aish/captures/'
    )
    
    args = parser.parse_args()
    
    # Create shell instance
    shell = AIShell(
        rhetor_endpoint=args.rhetor,
        debug=args.debug,
        capture=args.capture
    )
    
    # Handle general help command
    if args.ai_or_script == "help":
        print("Usage: aish [options] [component] [command/message]")
        tekton_root = TektonEnviron.get('TEKTON_ROOT', '/Users/cskoons/projects/github/Tekton')
        print(f"AI Training: {tekton_root}/MetaData/TektonDocumentation/AITraining/aish/")
        print(f"User Guides: {tekton_root}/MetaData/TektonDocumentation/UserGuides/aish/")
        return
    
    # Handle special commands
    if args.list_ais:
        shell._list_ais()
        return
    
    # Known AI names with their roles (all lowercase for comparison)
    ai_names = ['numa',        # Companion
                'tekton',      # Projects
                'prometheus',  # Planning
                'telos',       # Requirements
                'metis',       # Workflows
                'harmonia',    # Orchestration
                'synthesis',   # Integration
                'athena',      # Knowledge
                'sophia',      # Learning
                'noesis',      # Discovery
                'engram',      # Memory
                'apollo',      # Attention/Prediction
                'rhetor',      # LLM/Prompt/Context
                'penia',       # LLM Cost
                'hermes',      # Messages/Data
                'ergon',       # Agents/Tools/MCP
                'terma',       # Terminal
                'hephaestus',  # UI/DevTools
                'team-chat']   # Broadcast to all
    
    # Handle special commands first
    if args.ai_or_script == 'whoami':
        # Show terminal name from environment
        terminal_name = TektonEnviron.get('TERMA_TERMINAL_NAME', 'Not in terma')
        print(terminal_name)
        return
    
    if args.ai_or_script == 'list':
        # Handle list commands
        if args.message and len(args.message) > 0 and args.message[0] == 'commands':
            # Show command reference
            print("aish Command Reference")
            print("=" * 50)
            print()
            print("CORE COMMANDS:")
            print("  aish                           Show help")
            print("  aish whoami                    Show identity information")
            print("  aish list                      List available AI components")
            print("  aish list commands             Show this command reference")
            print("  aish <ai> \"message\"            Send message to AI")
            print()
            print("FORWARDING:")
            print("  aish forward <ai> <terminal>   Forward AI messages to terminal")
            print("  aish forward list              Show active forwards")
            print("  aish forward remove <ai>       Remove forwarding")
            print("  aish unforward <ai>            Remove forwarding (alternative)")
            print()
            print("PURPOSE:")
            print("  aish purpose                   Get terminal purpose: context for current terminal session")
            print("  aish purpose <name>            Get short form terminal purpose of <name>")
            print("  aish purpose <name> \"roles\"    Set terminal purpose/roles")
            print()
            print("PROJECTS:")
            print("  aish project list              List Tekton managed projects with CI info")
            print("  aish project forward <name>    Forward project CI to current terminal")
            print("  aish project forward <name> <terminal>  Forward project CI to specific terminal")
            print("  aish project unforward <name>  Remove project CI forwarding")
            print()
            print("INTELLIGENT ROUTES:")
            print("  aish route name \"review\" numa \"prep\" apollo \"analyze\"  Define named route")
            print("  aish route send \"review\" \"message\"     Send through route")
            print("  aish route list                Show all routes")
            print("  aish route show \"review\"       Show route details")
            print("  aish route remove \"review\"     Remove route")
            print()
            print("TERMINAL:")
            print("  aish terma inbox               Show terminal inbox")
            print("  aish terma send <name> \"msg\"   Send inter-terminal message")
            print("  aish terma broadcast \"msg\"     Broadcast to all terminals")
            print("  aish terma inbox               Show terminal inbox")
            print()
            print("SESSION CAPTURE:")
            print("  aish review start              Start recording terminal session")
            print("  aish review stop               Stop recording and save session")
            print("  aish review list               List recent session recordings")
            print()
            print("PRODUCTIVITY:")
            print("  autoprompt start               Keep Claude CI active (default: 2s)")
            print("  autoprompt stop                Stop autoprompt")
            print("  autoprompt status              Check autoprompt status")
            print()
            print("CLAUDE CODE IDE (yes Claude, this really is for you!):")
            print("  aish introspect <class>        See all methods of a class - no more guessing!")
            print("  aish context <file.py>         Show available classes in file scope")
            print("  aish explain \"error msg\"       Get help fixing Python errors")
            print()
            print("  CIs: You're encouraged to use these tools liberally. If you find patterns")
            print("  that could be improved or new commands that would help, please suggest them!")
            print()
            print("STATUS & DEBUG:")
            print("  aish status                    Show forwarding, AIs, and terminal status")
            print("  aish status --json             Machine-readable status output")
            print("  aish status mcp                Check MCP server status")
            print("  aish restart                   Restart MCP server")
            print("  aish logs                      Show MCP server logs")
            print("  aish debug-mcp                 Toggle MCP debug mode")
            print()
            print("ADVANCED:")
            print("  echo \"msg\" | aish <ai>         Pipe input to AI")
            print("  aish <ai1> | aish <ai2>        Chain AI responses")
            print("  aish team-chat \"message\"       Broadcast to all AIs")
            print()
            tekton_root = TektonEnviron.get('TEKTON_ROOT', '/Users/cskoons/projects/github/Tekton')
            print(f"Full documentation: {tekton_root}/MetaData/TektonDocumentation/AITraining/aish/COMMAND_REFERENCE.md")
            return
        else:
            # Show available AIs
            print("Available AI Components:")
            print()
            print("Core AIs:")
            print("  numa         - Companion AIi for Tekton Project")
            print("  tekton       - Tekton Multi-AI Engineering Platform")
            print("  prometheus   - Forward planning and foresight")
            print("  telos        - Purpose and completion")
            print("  metis        - Analysis and insight")
            print("  harmonia     - Balance and system harmony")
            print("  synthesis    - Integration and coordination")
            print("  athena       - Strategic wisdom and decision making")
            print("  sophia       - Wisdom and knowledge")
            print("  noesis       - Understanding and comprehension")
            print("  engram       - Memory and persistence")
            print("  apollo       - Predictive intelligence and attention")
            print("  rhetor       - Communication and prompt optimization")
            print("  penia        - Resource management")
            print("  hermes       - Messaging and communication")
            print("  ergon        - Work execution and tools")
            print()
            print("Infrastructure:")
            print("  terma        - Terminal management")
            print("  hephaestus   - User interface")
            print()
            print("Special:")
            print("  team-chat    - Broadcast to all AIs")
            print("  project      - Managed Project CIs")
            print()
            print("For all commands: aish list commands")
            return
    
    if args.ai_or_script == 'forward':
        return _dispatch_command('forward', args, shell)
    
    if args.ai_or_script == 'unforward':
        return _dispatch_command('unforward', args, shell)
    
    if args.ai_or_script == 'prompt':
        return _dispatch_command('prompt', args, shell)
    
    if args.ai_or_script == 'purpose':
        return _dispatch_command('purpose', args, shell)
    
    if args.ai_or_script == 'review':
        return _dispatch_command('review', args, shell)
    
    if args.ai_or_script == 'project':
        return _dispatch_command('project', args, shell)
    
    if args.ai_or_script == 'route':
        return _dispatch_command('route', args, shell)
    
    # Claude Code IDE commands
    if args.ai_or_script == 'introspect':
        return _dispatch_command('introspect', args, shell)
    
    if args.ai_or_script == 'context':
        return _dispatch_command('context', args, shell)
    
    if args.ai_or_script == 'explain':
        return _dispatch_command('explain', args, shell)
    
    # Status commands
    if args.ai_or_script == 'status':
        if args.message and len(args.message) > 0 and args.message[0] == 'mcp':
            # MCP status
            shell._check_mcp_status()
        else:
            # General aish status
            from commands.status import handle_status_command
            # Pass remaining args which might include --json
            handle_status_command(args.message if args.message else [])
        return
    
    if args.ai_or_script == 'restart':
        shell._restart_mcp_server()
        return
    
    if args.ai_or_script == 'logs':
        shell._show_mcp_logs()
        return
    
    if args.ai_or_script == 'debug-mcp':
        shell._toggle_mcp_debug()
        return
    
    # Check if first argument is an AI name (case-insensitive)
    if args.ai_or_script and args.ai_or_script.lower() in ai_names:
        return _handle_ai_command(args, shell, ai_names)
    
    # Original behavior for other cases
    elif args.command:
        # Single command mode
        shell.execute_command(args.command)
    elif args.ai_or_script and os.path.isfile(args.ai_or_script):
        # Script execution mode
        shell.execute_script(args.ai_or_script)
    elif args.ai_or_script:
        # Unknown AI or command
        print(f"Error: Unknown AI or command: {args.ai_or_script}", file=sys.stderr)
        print("", file=sys.stderr)
        print("Available AIs: " + ", ".join(ai_names), file=sys.stderr)
        print("", file=sys.stderr)
        print("For all commands: aish list commands", file=sys.stderr)
        sys.exit(1)
    else:
        # No arguments - show help
        print("aish - AI Shell for Tekton")
        print()
        print("Usage: aish <ai-name> \"message\"")
        print()
        print("Quick Start:")
        print("  aish numa \"Hello\"          Send message to AI")
        print("  aish list                  Show available AIs")
        print("  aish list commands         Show all commands ← START HERE")
        print("  aish purpose               Your true purpose: context for this terminal session")
        print("  autoprompt start           Keep Claude active (CI essential!)")
        print()
        tekton_root = TektonEnviron.get('TEKTON_ROOT', '/Users/cskoons/projects/github/Tekton')
        print(f"Documentation: {tekton_root}/MetaData/TektonDocumentation/AITraining/aish/COMMAND_REFERENCE.md")
        print()
        print("AI Documentation:")
        print(f"  AI Training: {tekton_root}/MetaData/TektonDocumentation/AITraining/")
        print(f"  User Guides: {tekton_root}/MetaData/TektonDocumentation/UserGuides/")

if __name__ == '__main__':
    main()
