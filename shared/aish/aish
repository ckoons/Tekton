#!/usr/bin/env python3
"""
aish - The AI Shell
Main entry point for the AI shell interpreter.
"""

import sys
import os
import argparse
from pathlib import Path


# Get the real path of the script (follows symlinks)
script_path = Path(__file__).resolve()
aish_root = script_path.parent
src_path = aish_root / 'src'

# Add src to path
sys.path.insert(0, str(src_path))

# Also add Tekton root if available (we're inside Tekton/shared/aish)
tekton_root = aish_root.parent.parent  # Go up to Tekton root
if tekton_root.exists() and (tekton_root / 'shared').exists():
    sys.path.insert(0, str(tekton_root))

from core.shell import AIShell
from core.version import __version__

def main():
    parser = argparse.ArgumentParser(
        description='aish - The AI Shell\n\nProvides AI capabilities in your terminal through the Tekton platform.',
        epilog='''Examples:
  aish                      # Start interactive AI shell
  aish apollo "question"    # Send question to Apollo AI
  echo "data" | aish athena # Pipe data to Athena AI
  aish team-chat "hello"    # Broadcast to all AIs
  aish -l                   # List available AIs
  aish script.ai            # Run AI script file

Available AIs: numa, tekton, prometheus, telos, metis, harmonia,
               synthesis, athena, sophia, noesis, engram, apollo,
               rhetor, penia, hermes, ergon, terma

Environment variables:
  RHETOR_ENDPOINT    # Override default Rhetor endpoint
  AISH_ACTIVE        # Set by aish-proxy when active
  AISH_DEBUG         # Enable debug output''',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument(
        'ai_or_script',
        nargs='?',
        help='AI name (apollo, athena, etc.) or script file'
    )
    
    parser.add_argument(
        'message',
        nargs='*',
        help='Message to send to AI'
    )
    
    parser.add_argument(
        '-c', '--command',
        help='Execute a single AI pipeline command and exit'
    )
    
    parser.add_argument(
        '-v', '--version',
        action='version',
        version=f'aish {__version__}'
    )
    
    parser.add_argument(
        '-d', '--debug',
        action='store_true',
        help='Enable debug output'
    )
    
    parser.add_argument(
        '-r', '--rhetor',
        default=None,
        help='Rhetor endpoint (default: TEKTON_RHETOR_PORT env var or http://localhost:8003)'
    )
    
    parser.add_argument(
        '-l', '--list-ais',
        action='store_true',
        help='List available AI specialists and exit'
    )
    
    args = parser.parse_args()
    
    # Create shell instance
    shell = AIShell(
        rhetor_endpoint=args.rhetor,
        debug=args.debug
    )
    
    # Handle general help command
    if args.ai_or_script == "help":
        print("Usage: aish [options] [component] [command/message]")
        tekton_root = os.environ.get('TEKTON_ROOT', '/Users/cskoons/projects/github/Tekton')
        print(f"AI Training: {tekton_root}/MetaData/TektonDocumentation/AITraining/aish/")
        print(f"User Guides: {tekton_root}/MetaData/TektonDocumentation/UserGuides/aish/")
        return
    
    # Handle special commands
    if args.list_ais:
        shell._list_ais()
        return
    
    # Known AI names with their roles (all lowercase for comparison)
    ai_names = ['numa',        # Companion
                'tekton',      # Projects
                'prometheus',  # Planning
                'telos',       # Requirements
                'metis',       # Workflows
                'harmonia',    # Orchestration
                'synthesis',   # Integration
                'athena',      # Knowledge
                'sophia',      # Learning
                'noesis',      # Discovery
                'engram',      # Memory
                'apollo',      # Attention/Prediction
                'rhetor',      # LLM/Prompt/Context
                'penia',       # LLM Cost
                'hermes',      # Messages/Data
                'ergon',       # Agents/Tools/MCP
                'terma',       # Terminal
                'hephaestus',  # UI/DevTools
                'team-chat']   # Broadcast to all
    
    # Handle special commands first
    if args.ai_or_script == 'whoami':
        # Show terminal name from environment
        terminal_name = os.environ.get('TERMA_TERMINAL_NAME', 'Not in terma')
        print(terminal_name)
        return
    
    if args.ai_or_script == 'list':
        # Handle list commands
        if args.message and len(args.message) > 0 and args.message[0] == 'commands':
            # Show command reference
            print("aish Command Reference")
            print("=" * 50)
            print()
            print("CORE COMMANDS:")
            print("  aish                           Show help")
            print("  aish whoami                    Show identity information")
            print("  aish list                      List available AI components")
            print("  aish list commands             Show this command reference")
            print("  aish <ai> \"message\"            Send message to AI")
            print()
            print("FORWARDING:")
            print("  aish forward <ai> <terminal>   Forward AI messages to terminal")
            print("  aish forward list              Show active forwards")
            print("  aish forward remove <ai>       Remove forwarding")
            print("  aish unforward <ai>            Remove forwarding (alternative)")
            print()
            print("TERMINAL:")
            print("  aish terma inbox               Show terminal inbox")
            print("  aish terma send <name> \"msg\"   Send inter-terminal message")
            print("  aish terma broadcast \"msg\"     Broadcast to all terminals")
            print("  aish purpose                   Get terminal purpose: context for currentteminal session")
            print("  aish purpose <name>            Get short form terminal purpose of <name>")
            print("  aish purpose <name> \"roles\"    Set terminal purpose/roles")
            print()
            print("PRODUCTIVITY:")
            print("  autoprompt start               Keep Claude CI active (default: 2s)")
            print("  autoprompt stop                Stop autoprompt")
            print("  autoprompt status              Check autoprompt status")
            print()
            print("ADVANCED:")
            print("  echo \"msg\" | aish <ai>         Pipe input to AI")
            print("  aish <ai1> | aish <ai2>        Chain AI responses")
            print("  aish team-chat \"message\"       Broadcast to all AIs")
            print()
            tekton_root = os.environ.get('TEKTON_ROOT', '/Users/cskoons/projects/github/Tekton')
            print(f"Full documentation: {tekton_root}/MetaData/TektonDocumentation/AITraining/aish/COMMAND_REFERENCE.md")
            return
        else:
            # Show available AIs
            print("Available AI Components:")
            print()
            print("Core AIs:")
            print("  numa         - Practical implementation and engineering")
            print("  prometheus   - Forward planning and foresight")
            print("  athena       - Strategic wisdom and decision making")
            print("  synthesis    - Integration and coordination")
            print()
            print("Specialized AIs:")
            print("  apollo       - Predictive intelligence and attention")
            print("  rhetor       - Communication and prompt optimization")
            print("  metis        - Analysis and insight")
            print("  harmonia     - Balance and system harmony")
            print("  noesis       - Understanding and comprehension")
            print("  engram       - Memory and persistence")
            print("  penia        - Resource management")
            print("  hermes       - Messaging and communication")
            print("  ergon        - Work execution and tools")
            print("  sophia       - Wisdom and knowledge")
            print("  telos        - Purpose and completion")
            print()
            print("Infrastructure:")
            print("  terma        - Terminal management")
            print("  hephaestus   - User interface")
            print()
            print("Special:")
            print("  team-chat    - Broadcast to all AIs")
            print()
            print("For all commands: aish list commands")
            return
    
    if args.ai_or_script == 'forward':
        from commands.forward import handle_forward_command
        return handle_forward_command(args.message or [])
    
    if args.ai_or_script == 'unforward':
        from commands.forward import handle_unforward_command
        return handle_unforward_command(args.message or [])
    
    if args.ai_or_script == 'prompt':
        from commands.prompt import handle_prompt_command
        return handle_prompt_command(args.message or [])
    
    if args.ai_or_script == 'purpose':
        from commands.purpose import handle_purpose_command
        return handle_purpose_command(args.message or [])
    
    # Check if first argument is an AI name (case-insensitive)
    if args.ai_or_script and args.ai_or_script.lower() in ai_names:
        component = args.ai_or_script.lower()
        
        
        # Special handling for terma commands
        if component == 'terma':
            # Import and use terma handler
            from commands.terma import handle_terma_command
            return handle_terma_command(args.message)
        
        # Check for help command
        if args.message and args.message[0] == "help":
            print(f"Usage: aish {component} [message]")
            tekton_root = os.environ.get('TEKTON_ROOT', '/Users/cskoons/projects/github/Tekton')
            print(f"AI Training: {tekton_root}/MetaData/TektonDocumentation/AITraining/{component}/")
            print(f"User Guides: {tekton_root}/MetaData/TektonDocumentation/UserGuides/{component}/")
            return
        
        
        # Get input from message args first, then stdin, then interactive
        if args.message:
            # Message provided as arguments - this takes priority
            input_data = ' '.join(args.message).strip()
            if args.debug:
                print(f"[DEBUG] Using message args: {args.message}")
                print(f"[DEBUG] Joined as: '{input_data}'")
        elif not sys.stdin.isatty():
            # Data might be piped in
            # Try to read with a very short timeout to avoid hanging
            import select
            if select.select([sys.stdin], [], [], 0.0)[0]:
                input_data = sys.stdin.read().strip()
                if args.debug:
                    print(f"[DEBUG] Got piped input: '{input_data}'")
            else:
                # No data in pipe, treat as interactive
                print(f"[aish] Entering interactive mode with {component}")
                print("Type your message and press Ctrl+D when done:")
                input_data = sys.stdin.read().strip()
        else:
            # Interactive mode with specific AI
            print(f"[aish] Entering interactive mode with {component}")
            print("Type your message and press Ctrl+D when done:")
            input_data = sys.stdin.read().strip()
        
        # Check if we actually have data
        if not input_data:
            print(f"aish: No message provided for {component}", file=sys.stderr)
            sys.exit(1)
        
        # Send directly to AI (no synthetic pipeline)
        if component == 'team-chat':
            shell.broadcast_message(input_data)
        else:
            shell.send_to_ai(component, input_data)
    
    # Original behavior for other cases
    elif args.command:
        # Single command mode
        shell.execute_command(args.command)
    elif args.ai_or_script and os.path.isfile(args.ai_or_script):
        # Script execution mode
        shell.execute_script(args.ai_or_script)
    elif args.ai_or_script:
        # Unknown AI or command
        print(f"Error: Unknown AI or command: {args.ai_or_script}", file=sys.stderr)
        print("", file=sys.stderr)
        print("Available AIs: " + ", ".join(ai_names), file=sys.stderr)
        print("", file=sys.stderr)
        print("For all commands: aish list commands", file=sys.stderr)
        sys.exit(1)
    else:
        # No arguments - show help
        print("aish - AI Shell for Tekton")
        print()
        print("Usage: aish <ai-name> \"message\"")
        print()
        print("Quick Start:")
        print("  aish numa \"Hello\"          Send message to AI")
        print("  aish list                  Show available AIs")
        print("  aish list commands         Show all commands ‚Üê START HERE")
        print("  aish purpose               Your true purpose: context for this terminal session")
        print("  autoprompt start           Keep Claude active (CI essential!)")
        print()
        tekton_root = os.environ.get('TEKTON_ROOT', '/Users/cskoons/projects/github/Tekton')
        print(f"Documentation: {tekton_root}/MetaData/TektonDocumentation/AITraining/aish/COMMAND_REFERENCE.md")
        print()
        print("AI Documentation:")
        print(f"  AI Training: {tekton_root}/MetaData/TektonDocumentation/AITraining/")
        print(f"  User Guides: {tekton_root}/MetaData/TektonDocumentation/UserGuides/")

if __name__ == '__main__':
    main()
